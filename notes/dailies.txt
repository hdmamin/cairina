2/1/23 wed
----------
X -settle on project
X -find all old relevant work
    > added 3 relevant files to this dir for now. The fourth (and perhaps most developed) effort is in the ~/nbextension-duckling dir.
~ -decide how to structure dir(s)/repo(s) (Will prob have multiple semi-related outputs, perhaps 1-5.)
    > Tentatively thinking monorepo. Easier to track everything that way.
-pick repo name
    > cairina, from "cairina moschata", one of the biggest, most powerful looking species of duck ("muscovy"). Think (rubber) duck on steroids.
X -add readme img
    > :D
~ -read distill AI-augmented human intelligence post

2/2/23 thurs
------------
X -premortem
X -map out very tentative plan for order to take things on (e.g. debugger -> jupyter magic -> interactive error messages -> nbextension -> reloading stuff -> LLM native programming language)
    > Lets start with debugger, then jupyter magic. The former is pretty far along and the latter should largely build on top of the debugger. We can take stock after that - depending on timing, can take a break or fit in error messages before break or move on to one of the bigger rocks (live reloading stuff or nbextension) if it went surprisingly fast.
X -consider whether notebook extension proj should continue or start over as jupyterlab extension
    -find stats on how commonly used each is?
        > In 2019 jetbrains python dev survey, DS use cases skewed twoards notebook (13% vs 5%; if we normalize to avoid non-jupyter options, that becomes 72% and 28%). By 2020 survey, it was 11% vs 6% (65% and 35% normalized). In 2021, they didn't break down results by DS users, so it's just 3% vs 2% (60% vs 40% normalized). In the "additional but not main IDE" question, it was 25% vs 12% notebooks (68% vs 32% normalized). In kaggle 2022 survey, 66.5% used nb while 23.7% used lab (74% vs 26% normalized). So (somewhat surprisingly) it seems like notebook is still considerably more popular, though perhaps slowly shifting towards lab. I like nb and started building in that so it's tempting to continue in that, but I guess I don't need to decide quite yet given that we're starting with the debugger.
~ -create cookiecutter template for each major project
    > Just start with debugger subdir (now "roboduck", i.e. a more powerful rubber duck). Uploaded to pypi to claim name.
-finish distill AI-augmented human intelligence post

2/3/23 fri
----------
X -remind myself where I left off w/ debugger work
    X -run existing code (do)
    -skim over existing code (read)
    X -read through done and outstanding todos
    X -write some todo items for what comes next
X -possible bug: seems like if we haven't saved nb since latest changes, load_ipynb func can load an outdated version (possibly? Though tit did that once but then I looked again and it seemed to be up to date so idk)
    > Confirmed that was happening but fixed it w/ help of a stackoverflow function.
X -consider adding option for "I don't know". Or maybe something like "If you don't know what's causing the bug, say "I don't know". Then write a list of 5 plausible causes that the developer can check for when debugging." (take advantage of its strength at generating lists, thinking of possibilities we might not)
X -update prompt to explain to gpt what the chat_db()/roboduck()/rbd() func call is
X -rename usages from llmdb to rdb or similar?
X -update func names to fit roboduck lib
X -add stop word to prompt to prevent starting another question
-finish distill AI-augmented human intelligence post

2/4/23 sat
----------
X -see if we can color user/model turns differently. (Bit hard to read atm, though partly bc I'm printing out full prompt for debugging purposes.)
X -add backup value when response is nothing
X -debug why I'm getting so many empty responses
    > Turns out I had docstring quotes in my stop phrases which can be problematic since that is a valid thing to generate at times. Also didn't start my prompt with docstring quotes which maybe degrades quality a bit. But if I add them back in, then I feel like I need it in stopwords again. Hmmm.
X -make codex respond in second person
X -figure out how to make multiline str start with quotes in yaml (or abandon their use)
    > Turns out my old method already supports that. Just don't escape them.
-consider tweaking prompt to use proxy/authority (e.g. "Answer Key")
-hide user warning about using codex model name.

2/5/23 sun
----------
X -consider removing stop seq """ from 3 debug prompts (in case it actually needs to generate that w/in solution)
    > Yes, try it out. Just have roboduck rstrip them.
    > Update: with this format, codex seems to like closing the docstring before doing anything else, then jumping straight to code, meaning no explanation gets written. May need to revert to old no-docstring method.
    > Also realized I haven't been bumping the prompt versions. Whoops. I guess maybe this only needs to be done when I actually bump the jabberwocky pypi version and/or the roboduck pypi version.
_ -consider tweaking prompt to use proxy/authority (e.g. "Answer Key")
    > I think for now let's leave this be - I'll be seeing a lot of query results over the next couple months so it will become apparent if this is something we need (i.e. if quality is an issue). I also suspect we may see a chatgpt/gpt4/codex2 release that makes this type of prompt engineering less necessary before the end of this project, so no need to optimize that dimension now.
X -hide user warning about using codex model name
-consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.

2/6/23 mon
----------
X -try reverting debug prompts to no opening docstring (see sun notes for why)
    > Nope, tried this again but it really seems like it needs the docstring otherwise it just jumps straight to code. Added logit bias instead of stopword. Also add another stopword since I observed some cases where it skipped repeating the DEVELOPER QUESTION but did repeat LOCAL VARIABLES and everything afterwards.
X -see how chatGPT does
    > As expected, noticeably better. Codex seems to like repeating things and writing lengthy answers here even though I added the word "concise" to the instructions. I bet a better code api will become available within the next few months.
-consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.
X -stream completion instead of waiting for whole thing
    > Added streaming with .02s between characters. Maminbot used .015 but somehow even .02 seems really fast in jupyter. Maybe streamlit had extra latency so it wasn't really .015s there.

2/7/23 tues
-----------
~ -consider how to handle huge data structures (big df, long list, etc.) ~ - See if we can get this to work like ipdb where you can call it only AFTER an error occurs.
    X -write draft of truncated_repr func
    -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it wouuld be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/8/23 wed
-----------
X -add file from jabberwocky (I think?) to avoid counting jupyter notebooks in github code % count
-handle huge data structures (big df, long list, etc.) 
    X -handle case where seq[:1] still produces repr that's too long
    X -test behavior on dfs again (potential opportunity to do something custom like "pd.DataFrame([col1, col2, ...])"? As in literally that string, just call truncated_repr on cols.)
    -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it wouuld be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/9/23 thurs
-----------
~ -handle huge data structures (big df, long list, etc.) 
    X -consider desired behavior for cases like DotDict (might be nice to indicate to gpt that it's not just a regular dict, but not sure if there's any good indicator that allows us to identify that automatically - just "feels" like it would be useful info to provide. One option is to provide types for all values, though that would make the prompt snippet look less like valid python.)
        > I guess I intentionally made dotdict have the same str and repr as a regular dict so it's tough to come up with a rule that would help here. I could change its str/repr, just leave it as the regular dict version, or update my debug prompt to show the type of each var too (could be a separate section like GLOBALS_TYPES, could be a comment after each value in GLOBALS like `'nums': [1, 3, 4...], # list`. Last idea is interesting.
    X -try out idea about specifying var types in comments
        > Wrote new func. I like this.
    X -fix bug: empty sets represented wrong
        > Because empty lists and tuples use literals like [] but sets use 'set()' so my logic around the last non-brace index failed. Added some hardcoded logic around setes.
    X -fix bug: dict-like objects have extra quotes
        > Must have added this mistakenly, fixed now.
    X -tweak debug prompt to acknowledge that roboduck func may not be present (in case we repurpose these)
    -document (mention that length is more of a rough guideline, not strictly enforced)
    -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
-consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
-maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
-try running magic again and see if problems arise

2/10/23 fri
-----------
X -handle huge data structures (big df, long list, etc.) 
    X -integrate into RoboDuckDB cls (autocompute max len per var based on number of vars? Maybe only if original prompt is too long? Should I try to identify problematic vars and just shorten those dramatically (e.g. the 1 giant df or tensor) rather than risk truncating a bunch of medium size vars slightly?)
        > Just hardcode for now. There's not going to be some magic number here that's perfect.
    X -document (mention that length is more of a rough guideline, not strictly enforced)
_ -consider changing logic for when to call codex (currently when "?" is in text, but should it be more formally defined?)
    > Actually I think this is pretty good.
X -tweak prompts to hopefully reduce repetition a bit (been noticing this more lately)
    > Added frequency_penalty of 0.2 (pretty minor change for now, range is -2 to 2). Haven't tested yet, just keep an eye on things as I continue working on it. Don't want to sink too much time into selecting perfect prompt params now bc I think better models may be available soon.
~ -maybe start refactoring some things out of RoboDuckDB class to reduce overhead per call (e.g. load prompts once rather than every time)
    > Tried replacing load_prompt w/ PromptManager and realized the issue is that I wnat to check the resolved prompt, and pm.query() with verbose mode just prints. But found pm.prompt() and pm.kwargs() methods so it is possible. However, we have to pass GPT obj to PM on instantiation so if we want to support diff backends in roboduck init, we still need to create PM inside it. So not really saving any time. But maybe this work will help pave the way if I want to transition to ConversationManager at some point.
    > Haven't tested this yet but it's ready to be tried. I kept the old cls around in case I want to revert (w/out messing w/ jupyter/git interaction weirdness).
-try running magic again and see if problems arise

2/11/23 sat
-----------
-try running new version of roboduck with PM (just run all cells from cls def down to debugging session cell)
-consider whether to try replacing PM with convmanager
-if yes ^, briefly prototype some bios (should this be a strong programmer? Or something like "the python debugger/interpreter"? Or "a helpful AI code assistant embedded in the interpreter"?)
-try running jupyter magic again and see if problems arise


Backlog
-------
-lmdb nb
    -consider if there's a good way to make this more conversational in case we need to ask multiple questions. If we just print gpt's response, this won't work so well. Could try to revise this to fit into ConvManager paradigm.
    -debug slowness when using magic (is it calling query multiple times?) ~ - add option to add new cell w/ gpt-fixed function below (may need to adjust prompt a bit to encourage it to provide this)
-watch bret victor Future of Programming vid
-read Engelbart paper on augmenting human intellect

Easy Backlog
------------
-add my jabberwocky pre-commit hook to avoid pushing api key
